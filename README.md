# [ICASSP'23] Visual onoma-to-wave: Environmental sound synthesis from visually represented onomatopoeia
Official implementation of [Visual onoma-to-wave: environmental sound synthesis from visual onomatopoeias and sound-source images](https://arxiv.org/abs/2210.09173) ( to appear *ICASSP 2023* ) .

## Demo
[[Audio samples]](https://sarulab-speech.github.io/demo_visual-onoma-to-wave/)



## Getting started
Codes and pre-trained models will be available soon.

## Code contributors
- Hien Ohnaka (National Institute of Technology, Tokuyama College, Japan.)
- [Shinnosuke Takamichi](https://sites.google.com/site/shinnosuketakamichi/home) (The University of Tokyo, Japan.)

## Reference
- [Onoma-to-wave: Environmental sound synthesis from onomatopoeic words](https://arxiv.org/abs/2102.05872)
- [HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis](https://arxiv.org/abs/2010.05646)
- [FastSpeech2 implementation](https://github.com/Wataru-Nakata/FastSpeech2-JSUT)
  - Part of our codes follows this implementation.